"""
Baselineå¯¹æ¯”å®éªŒ

å¯¹æ¯”æ–¹æ³•ï¼š
1. ç›´æ¥å­˜å‚¨ï¼ˆIdentity Mappingï¼‰
2. PCAé™ç»´
3. æˆ‘ä»¬çš„ä¿¡æ¯ç²’å­ç³»ç»Ÿ

ä½œè€…ï¼šåŒ—äº¬æ±‚ä¸€æ•°ç”Ÿç§‘æŠ€ä¸­å¿ƒ
"""

import torch
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import time
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

from information_oriented_system_v2 import InformationOrientedSystemV2


class DirectStorageBaseline:
    """Baseline 1: ç›´æ¥å­˜å‚¨ï¼ˆæœ€ç®€å•çš„baselineï¼‰"""
    
    def __init__(self):
        self.name = "Direct Storage"
        self.stored_data = None
    
    def process(self, data):
        """ç›´æ¥å­˜å‚¨"""
        start = time.time()
        self.stored_data = data.clone()
        process_time = (time.time() - start) * 1000
        
        return {
            'process_time': process_time,
            'compressed_size': data.numel(),  # æ²¡æœ‰å‹ç¼©
            'storage': self.stored_data
        }
    
    def reconstruct(self):
        """ç›´æ¥è¿”å›"""
        start = time.time()
        reconstructed = self.stored_data
        recon_time = (time.time() - start) * 1000
        
        return reconstructed, recon_time


class PCABaseline:
    """Baseline 2: PCAé™ç»´"""
    
    def __init__(self, n_components=64, fit_data=None):
        self.name = f"PCA (n={n_components})"
        self.n_components = n_components
        self.pca = PCA(n_components=n_components)
        self.fitted = False
        self.compressed = None
        self.mean = None
        self.original_shape = None
        
        # é¢„å…ˆfit
        if fit_data is not None:
            self.pca.fit(fit_data)
            self.fitted = True
    
    def process(self, data):
        """PCAé™ç»´"""
        start = time.time()
        
        # å±•å¹³
        self.original_shape = data.shape
        data_flat = data.flatten().cpu().numpy().reshape(1, -1)
        
        # PCA
        self.compressed = self.pca.transform(data_flat)
        
        process_time = (time.time() - start) * 1000
        
        return {
            'process_time': process_time,
            'compressed_size': self.compressed.size,
            'storage': self.compressed
        }
    
    def reconstruct(self):
        """PCAé‡æ„"""
        start = time.time()
        
        reconstructed_flat = self.pca.inverse_transform(self.compressed)
        reconstructed = torch.tensor(reconstructed_flat, dtype=torch.float32)
        reconstructed = reconstructed.reshape(self.original_shape)
        
        recon_time = (time.time() - start) * 1000
        
        return reconstructed, recon_time


class OurMethod:
    """æˆ‘ä»¬çš„ä¿¡æ¯ç²’å­ç³»ç»Ÿ"""
    
    def __init__(self, particle_size=4):
        self.name = f"Information Particle System (psize={particle_size})"
        self.system = InformationOrientedSystemV2(particle_size=particle_size)
        self.output = None
    
    def process(self, data):
        """ä¿¡æ¯ç²’å­åŒ–"""
        start = time.time()
        self.output = self.system.forward(data)
        process_time = (time.time() - start) * 1000
        
        # è®¡ç®—å­˜å‚¨å¤§å°ï¼ˆç²’å­æ•° Ã— 12ç»´ç‰¹å¾ + raw_contentï¼‰
        num_particles = self.output['num_particles']
        compressed_size = num_particles * 12  # 12ç»´ç‰¹å¾
        # raw_contentä»ç„¶éœ€è¦å­˜å‚¨ï¼Œä½†æœ‰ç»“æ„åŒ–ä¿¡æ¯
        
        return {
            'process_time': process_time,
            'compressed_size': compressed_size,
            'num_particles': num_particles,
            'num_groups': self.output['num_groups'],
            'avg_sif': self.output['avg_sif'],
            'storage': self.output
        }
    
    def reconstruct(self):
        """æ— æŸé‡æ„"""
        start = time.time()
        reconstructed = self.system.reconstruct(self.output)
        recon_time = (time.time() - start) * 1000
        
        return reconstructed, recon_time


def compare_methods(image, label, methods):
    """å¯¹æ¯”æ‰€æœ‰æ–¹æ³•"""
    
    results = {}
    
    for method in methods:
        # å¤„ç†
        process_result = method.process(image)
        
        # é‡æ„
        reconstructed, recon_time = method.reconstruct()
        
        # éªŒè¯
        mse = F.mse_loss(reconstructed, image).item()
        cos_sim = F.cosine_similarity(
            reconstructed.flatten(),
            image.flatten(),
            dim=0
        ).item()
        
        results[method.name] = {
            'process_time': process_result['process_time'],
            'recon_time': recon_time,
            'total_time': process_result['process_time'] + recon_time,
            'mse': mse,
            'cosine_sim': cos_sim,
            'compressed_size': process_result.get('compressed_size', 0),
            'num_particles': process_result.get('num_particles', 'N/A'),
            'num_groups': process_result.get('num_groups', 'N/A'),
            'avg_sif': process_result.get('avg_sif', 'N/A'),
            'perfect': mse < 1e-6
        }
    
    return results


def run_comparison(num_samples=50, dataset='mnist'):
    """è¿è¡Œå®Œæ•´å¯¹æ¯”å®éªŒ"""
    
    print("="*70)
    print(f"  Baselineå¯¹æ¯”å®éªŒ - {dataset.upper()}")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    transform = transforms.Compose([transforms.ToTensor()])
    
    if dataset == 'mnist':
        testset = torchvision.datasets.MNIST(
            root='./data/MNIST', train=False, download=False, transform=transform
        )
    elif dataset == 'fashion':
        testset = torchvision.datasets.FashionMNIST(
            root='./data/FashionMNIST', train=False, download=False, transform=transform
        )
    
    # å‡†å¤‡PCAè®­ç»ƒæ•°æ®ï¼ˆç”¨å‰200ä¸ªæ ·æœ¬ï¼‰
    print("\nå‡†å¤‡PCAè®­ç»ƒæ•°æ®...")
    pca_train_data = []
    for i in range(min(200, len(testset))):
        img, _ = testset[i]
        pca_train_data.append(img.squeeze(0).flatten().numpy())
    pca_train_data = np.array(pca_train_data)
    print(f"  PCAè®­ç»ƒæ•°æ®å½¢çŠ¶: {pca_train_data.shape}")
    
    # åˆ›å»ºæ–¹æ³•
    methods = [
        DirectStorageBaseline(),
        PCABaseline(n_components=64, fit_data=pca_train_data),
        PCABaseline(n_components=128, fit_data=pca_train_data),
        OurMethod(particle_size=4)
    ]
    
    print(f"\nå¯¹æ¯”æ–¹æ³•:")
    for i, m in enumerate(methods):
        print(f"  {i+1}. {m.name}")
    
    # æµ‹è¯•
    print(f"\næµ‹è¯• {num_samples} ä¸ªæ ·æœ¬...")
    
    all_results = {m.name: [] for m in methods}
    
    for i in range(num_samples):
        image, label = testset[i]
        image_2d = image.squeeze(0)
        
        results = compare_methods(image_2d, label, methods)
        
        for method_name, result in results.items():
            all_results[method_name].append(result)
        
        if (i+1) % 10 == 0:
            print(f"  å¤„ç†: {i+1}/{num_samples}")
    
    # ç»Ÿè®¡
    print("\n" + "="*70)
    print("ğŸ“Š ç»“æœæ±‡æ€»")
    print("="*70)
    
    print(f"\n{'æ–¹æ³•':<35} {'å¤„ç†(ms)':<12} {'é‡æ„(ms)':<12} {'MSE':<15} {'å®Œç¾ç‡':<10}")
    print("-"*70)
    
    for method_name in all_results:
        results = all_results[method_name]
        
        avg_process = np.mean([r['process_time'] for r in results])
        avg_recon = np.mean([r['recon_time'] for r in results])
        avg_mse = np.mean([r['mse'] for r in results])
        perfect_rate = sum([r['perfect'] for r in results]) / len(results)
        
        print(f"{method_name:<35} {avg_process:<12.2f} {avg_recon:<12.2f} "
              f"{avg_mse:<15.10f} {perfect_rate*100:<10.1f}%")
    
    # è¯¦ç»†å¯¹æ¯”
    print(f"\n" + "="*70)
    print("ğŸ” è¯¦ç»†å¯¹æ¯”")
    print("="*70)
    
    # æ‰¾å‡ºæˆ‘ä»¬çš„æ–¹æ³•
    our_results = all_results[[k for k in all_results if 'Information' in k][0]]
    direct_results = all_results['Direct Storage']
    pca64_results = all_results['PCA (n=64)']
    
    print(f"\n1. é‡æ„è´¨é‡å¯¹æ¯”:")
    print(f"   Direct Storage:  MSE = {np.mean([r['mse'] for r in direct_results]):.10f}")
    print(f"   PCA (n=64):      MSE = {np.mean([r['mse'] for r in pca64_results]):.10f}")
    print(f"   Ours:            MSE = {np.mean([r['mse'] for r in our_results]):.10f}")
    
    print(f"\n2. é¢å¤–ä¿¡æ¯:")
    if our_results[0]['num_particles'] != 'N/A':
        print(f"   å¹³å‡ç²’å­æ•°:  {np.mean([r['num_particles'] for r in our_results]):.1f}")
        print(f"   å¹³å‡ä¿¡æ¯ç»„:  {np.mean([r['num_groups'] for r in our_results]):.1f}")
        print(f"   å¹³å‡SIFå€¼:   {np.mean([r['avg_sif'] for r in our_results]):.4f}")
        print(f"   âœ… æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†é¢å¤–çš„ç»“æ„åŒ–ä¿¡æ¯ï¼")
    
    print(f"\n3. å…³é”®ä¼˜åŠ¿:")
    our_mse = np.mean([r['mse'] for r in our_results])
    pca_mse = np.mean([r['mse'] for r in pca64_results])
    
    if our_mse < 1e-6:
        print(f"   âœ… æˆ‘ä»¬å®ç°å®Œç¾é‡æ„ï¼ˆMSEâ‰ˆ0ï¼‰")
        print(f"   âœ… PCAæœ‰ä¿¡æ¯æŸå¤±ï¼ˆMSE={pca_mse:.6f}ï¼‰")
    
    print(f"   âœ… æˆ‘ä»¬æä¾›12ç»´å¯è§£é‡Šç‰¹å¾")
    print(f"   âœ… æˆ‘ä»¬æä¾›ä¿¡æ¯ç»„ç»“æ„")
    print(f"   âœ… æˆ‘ä»¬æä¾›SIFè´¨é‡è¯„ä¼°")
    
    return all_results


def visualize_comparison(all_results, save_path='baseline_comparison.png'):
    """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
    
    print(f"\nğŸ¨ ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    methods = list(all_results.keys())
    colors = ['gray', 'blue', 'cyan', 'red']
    
    # 1. MSEå¯¹æ¯”
    mse_values = [np.mean([r['mse'] for r in all_results[m]]) for m in methods]
    axes[0].bar(range(len(methods)), mse_values, color=colors)
    axes[0].set_xticks(range(len(methods)))
    axes[0].set_xticklabels([m.replace(' ', '\n') for m in methods], fontsize=8)
    axes[0].set_ylabel('MSE')
    axes[0].set_title('Reconstruction Error (MSE)')
    axes[0].set_yscale('log')
    axes[0].grid(True, alpha=0.3)
    
    # 2. å¤„ç†æ—¶é—´å¯¹æ¯”
    time_values = [np.mean([r['total_time'] for r in all_results[m]]) for m in methods]
    axes[1].bar(range(len(methods)), time_values, color=colors)
    axes[1].set_xticks(range(len(methods)))
    axes[1].set_xticklabels([m.replace(' ', '\n') for m in methods], fontsize=8)
    axes[1].set_ylabel('Time (ms)')
    axes[1].set_title('Total Processing Time')
    axes[1].grid(True, alpha=0.3)
    
    # 3. å®Œç¾é‡æ„ç‡
    perfect_rates = [sum([r['perfect'] for r in all_results[m]])/len(all_results[m])*100 
                     for m in methods]
    axes[2].bar(range(len(methods)), perfect_rates, color=colors)
    axes[2].set_xticks(range(len(methods)))
    axes[2].set_xticklabels([m.replace(' ', '\n') for m in methods], fontsize=8)
    axes[2].set_ylabel('Perfect Reconstruction Rate (%)')
    axes[2].set_title('Perfect Reconstruction Rate')
    axes[2].set_ylim([0, 105])
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"   âœ… ä¿å­˜åˆ°: {save_path}")
    plt.close()


if __name__ == '__main__':
    # MNISTæµ‹è¯•
    mnist_results = run_comparison(num_samples=50, dataset='mnist')
    visualize_comparison(mnist_results, 'baseline_comparison_mnist.png')
    
    print("\n" + "="*70)
    
    # Fashion-MNISTæµ‹è¯•
    fashion_results = run_comparison(num_samples=50, dataset='fashion')
    visualize_comparison(fashion_results, 'baseline_comparison_fashion.png')
    
    print("\n" + "="*70)
    print("  å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print("="*70)

